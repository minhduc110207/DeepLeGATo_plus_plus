{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DeepLeGATo++ Configuration - Colab L4\n",
                "Configuration optimized for Google Colab L4 GPU (24GB VRAM, Ada Lovelace architecture)\n",
                "\n",
                "**L4 Key specs:**\n",
                "- 24GB GDDR6 VRAM\n",
                "- FP16/BF16 support with good throughput\n",
                "- ~120 TFLOPS FP16 Tensor Core\n",
                "- More VRAM than T4 (16GB), less than A100 (40GB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import yaml\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = {\n",
                "    \"model\": {\n",
                "        \"name\": \"DeepLeGATo++\",\n",
                "        \"backbone\": {\n",
                "            \"type\": \"swinv2_base_window8_256\",  # Can use base model with 24GB\n",
                "            \"pretrained\": True,\n",
                "            \"embed_dim\": 128,\n",
                "            \"depths\": [2, 2, 18, 2],\n",
                "            \"num_heads\": [4, 8, 16, 32],\n",
                "            \"window_size\": 7,\n",
                "            \"gradient_checkpointing\": True,  # Saves ~30% VRAM, needed for base model on 24GB\n",
                "            \"drop_path_rate\": 0.2\n",
                "        },\n",
                "        \"npe\": {\n",
                "            \"num_flow_layers\": 16,\n",
                "            \"hidden_dim\": 512,\n",
                "            \"num_transforms\": 8,\n",
                "            \"num_blocks\": 4,\n",
                "            \"dropout\": 0.1\n",
                "        },\n",
                "        \"num_params\": 7  # mag, Re, n, q, PA, x, y\n",
                "    },\n",
                "    \"training\": {\n",
                "        \"batch_size\": 32,  # 24GB allows 32 with grad checkpointing; effective 64 with accum\n",
                "        \"accumulation_steps\": 2,  # Effective batch size: 64\n",
                "        \"max_epochs\": 100,\n",
                "        \"optimizer\": {\n",
                "            \"type\": \"AdamW\",\n",
                "            \"learning_rate\": 1.5e-4,  # Between T4 (1e-4) and A100 (2e-4), tuned for effective bs 64\n",
                "            \"weight_decay\": 0.05,\n",
                "            \"betas\": [0.9, 0.999]\n",
                "        },\n",
                "        \"scheduler\": {\n",
                "            \"type\": \"CosineAnnealingWarmRestarts\",\n",
                "            \"T_0\": 10,\n",
                "            \"T_mult\": 2,\n",
                "            \"eta_min\": 1.0e-6\n",
                "        },\n",
                "        \"precision\": \"16-mixed\",  # FP16 mixed precision, well-supported on L4\n",
                "        \"checkpoint_every_n_epochs\": 1,\n",
                "        \"save_to_drive\": True,\n",
                "        \"resume_from_latest\": True,\n",
                "        \"early_stopping_patience\": 15\n",
                "    },\n",
                "    \"data\": {\n",
                "        \"image_size\": 256,  # 24GB can handle 256 with grad checkpointing\n",
                "        \"num_workers\": 2,  # Colab has limited CPU cores\n",
                "        \"pin_memory\": True,\n",
                "        \"prefetch_factor\": 3,\n",
                "        \"priors\": {\n",
                "            \"magnitude\": [15.0, 28.0],\n",
                "            \"effective_radius\": [0.1, 10.0],  # arcsec\n",
                "            \"sersic_index\": [0.3, 8.0],\n",
                "            \"axis_ratio\": [0.1, 1.0],\n",
                "            \"position_angle\": [0.0, 180.0],  # degrees\n",
                "            \"center_offset\": [-5.0, 5.0]  # pixels\n",
                "        },\n",
                "        \"noise\": {\n",
                "            \"sky_background\": 0.01,\n",
                "            \"read_noise\": 5.0,\n",
                "            \"gain\": 2.5\n",
                "        },\n",
                "        \"drive_base\": \"/content/drive/MyDrive/DeepLeGATo++\",\n",
                "        \"num_train_samples\": 200000,\n",
                "        \"num_val_samples\": 20000,\n",
                "        \"generate_on_the_fly\": True\n",
                "    },\n",
                "    \"logging\": {\n",
                "        \"use_wandb\": True,\n",
                "        \"project_name\": \"DeepLeGATo++\",\n",
                "        \"log_every_n_steps\": 25,\n",
                "        \"log_images_every_n_epochs\": 5\n",
                "    },\n",
                "    \"validation\": {\n",
                "        \"val_check_interval\": 1.0,  # Every epoch\n",
                "        \"num_posterior_samples\": 200  # For uncertainty estimation\n",
                "    }\n",
                "}\n",
                "\n",
                "# Set derived paths\n",
                "config[\"data\"][\"train_data_path\"] = config[\"data\"][\"drive_base\"] + \"/data/train\"\n",
                "config[\"data\"][\"val_data_path\"] = config[\"data\"][\"drive_base\"] + \"/data/val\"\n",
                "config[\"data\"][\"checkpoint_path\"] = config[\"data\"][\"drive_base\"] + \"/checkpoints\"\n",
                "config[\"logging\"][\"log_dir\"] = config[\"data\"][\"drive_base\"] + \"/logs\"\n",
                "\n",
                "print(\"L4 Configuration loaded successfully!\")\n",
                "print(f\"  Backbone: {config['model']['backbone']['type']}\")\n",
                "print(f\"  Image size: {config['data']['image_size']}\")\n",
                "print(f\"  Batch size: {config['training']['batch_size']} x {config['training']['accumulation_steps']} accum = {config['training']['batch_size'] * config['training']['accumulation_steps']} effective\")\n",
                "print(f\"  Precision: {config['training']['precision']}\")\n",
                "print(f\"  Grad checkpointing: {config['model']['backbone']['gradient_checkpointing']}\")\n",
                "config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save as YAML if needed\n",
                "# with open('colab_l4.yaml', 'w') as f:\n",
                "#     yaml.dump(config, f, default_flow_style=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}